/**
 * Direct StarCoder2 Tokenizer - å®Œå…¨ç”¨ StarCoder2 å–ä»£ç¾æœ‰æ–¹æ¡ˆ
 * StarCoder2 å’å‹ï¼Œç›´æ¥ç”¨ä½¢å°±å¾—
 */

import { AutoTokenizer } from '@huggingface/transformers';

export interface DirectStarCoder2Token {
  text: string;
  id: number;
  score: number; // åŸºæ–¼é »ç‡å’Œé‡è¦æ€§çš„æœç´¢åˆ†æ•¸
  confidence: number;
  relevance: 'high' | 'medium' | 'low';
}

export interface DirectStarCoder2Result {
  tokens: DirectStarCoder2Token[];
  metadata: {
    totalTokens: number;
    vocabSize: number;
    processingTime: number;
    averageConfidence: number;
  };
  raw: {
    inputIds: number[];
    decodedText: string;
  };
}

/**
 * Direct StarCoder2 Tokenizer - å®Œå…¨åŸºæ–¼æ¥­ç•Œæœ€å¼·å˜…ä»£ç¢¼ç†è§£æ¨¡å‹
 */
export class DirectStarCoder2Tokenizer {
  private tokenizer: any;
  private initialized: boolean = false;
  private modelPath: string;

  constructor(options: {
    modelPath?: string;
  } = {}) {
    this.modelPath = options.modelPath || './models/starcoder2';
  }

  /**
   * åˆå§‹åŒ– StarCoder2 tokenizer
   */
  async initialize(): Promise<void> {
    if (this.initialized) return;

    console.log('ğŸš€ Initializing Direct StarCoder2 Tokenizer...');

    try {
      this.tokenizer = await AutoTokenizer.from_pretrained(this.modelPath);
      this.initialized = true;
      console.log('âœ… Direct StarCoder2 Tokenizer initialized successfully!');
    } catch (error) {
      console.error('âŒ Failed to initialize StarCoder2 tokenizer:', error);
      throw new Error(`StarCoder2 initialization failed: ${error.message}`);
    }
  }

  /**
   * ä¸»è¦ tokenization æ–¹æ³•
   */
  async tokenize(content: string): Promise<DirectStarCoder2Result> {
    if (!this.initialized) {
      await this.initialize();
    }

    const startTime = Date.now();
    console.log('ğŸ”¤ Starting Direct StarCoder2 tokenization...');

    try {
      // ä½¿ç”¨ StarCoder2 é€²è¡Œ tokenization
      const encoded = await this.tokenizer(content);
      const inputIds = encoded.input_ids.tolist()[0];

      // è§£ç¢¼ç²å¾—åŸæ–‡
      const decodedText = await this.tokenizer.decode(inputIds);

      // ä½¿ç”¨ decode çµæœä¾†å‰µå»ºæœ‰æ„ç¾©å˜… tokens
      const tokens = this.extractMeaningfulTokens(decodedText, inputIds, content);

      // è¨ˆç®—åˆ†æ•¸å’Œç›¸é—œæ€§
      const scoredTokens = this.calculateScores(tokens, content);

      const processingTime = Date.now() - startTime;
      console.log(`âœ… Direct StarCoder2 tokenization completed in ${processingTime}ms`);

      return {
        tokens: scoredTokens,
        metadata: {
          totalTokens: scoredTokens.length,
          vocabSize: 49152, // StarCoder2 è©å½™è¡¨å¤§å°
          processingTime,
          averageConfidence: this.calculateAverageConfidence(scoredTokens)
        },
        raw: {
          inputIds,
          decodedText
        }
      };

    } catch (error) {
      console.error('âŒ StarCoder2 tokenization failed:', error);
      throw new Error(`Tokenization failed: ${error.message}`);
    }
  }

  /**
   * å¾è§£ç¢¼æ–‡æœ¬ä¸­æå–æœ‰æ„ç¾©å˜… tokens
   */
  private extractMeaningfulTokens(decodedText: string, inputIds: number[], originalContent: string): DirectStarCoder2Token[] {
    const tokens: DirectStarCoder2Token[] = [];

    // æ–¹æ³•1: ä½¿ç”¨ç°¡å–®çš„åˆ†è©ï¼Œä½†ä¿ç•™æŠ€è¡“è¡“èª
    const words = decodedText
      .replace(/[{}()\[\];,<\.>]/g, ' $& ') // ç‚ºç¬¦è™Ÿå‘¨åœåŠ ç©ºæ ¼
      .split(/\s+/)
      .filter(word => word.length > 0);

    // ç‚ºæ¯å€‹è©å‰µå»º token
    let idIndex = 0;
    for (let i = 0; i < words.length; i++) {
      const word = words[i];

      // ç‚ºæ¯å€‹è©åˆ†é…ä¸€å€‹æˆ–å¤šå€‹ ID (ç°¡åŒ–è™•ç†)
      const tokenId = idIndex < inputIds.length ? inputIds[idIndex] : inputIds[inputIds.length - 1];
      idIndex++;

      tokens.push({
        text: word,
        id: tokenId,
        score: 0.5, // åŸºç¤åˆ†æ•¸
        confidence: 0.9,
        relevance: 'medium' as const
      });
    }

    // æ–¹æ³•2: æ·»åŠ ç‰¹æ®Šè­˜åˆ¥å˜…æŠ€è¡“è¡“èª
    const technicalTerms = this.extractTechnicalTerms(decodedText);
    for (const term of technicalTerms) {
      const existingToken = tokens.find(t => t.text.toLowerCase() === term.toLowerCase());
      if (existingToken) {
        // æå‡ç¾æœ‰æŠ€è¡“è¡“èªçš„åˆ†æ•¸
        existingToken.score = Math.min(existingToken.score + 0.3, 1.0);
        existingToken.confidence = Math.min(existingToken.confidence + 0.1, 1.0);
        existingToken.relevance = this.determineRelevance(existingToken.score);
      } else {
        // æ·»åŠ æ–°å˜…æŠ€è¡“è¡“èª token
        const tokenId = idIndex < inputIds.length ? inputIds[idIndex] : inputIds[inputIds.length - 1];
        idIndex++;
        tokens.push({
          text: term,
          id: tokenId,
          score: 0.8,
          confidence: 0.95,
          relevance: 'high' as const
        });
      }
    }

    return tokens;
  }

  /**
   * æå–æŠ€è¡“è¡“èª
   */
  private extractTechnicalTerms(text: string): string[] {
    const technicalTerms: string[] = [];

    // å¸¸è¦‹æŠ€è¡“è¡“èªæ¨¡å¼
    const patterns = [
      // å…¨å¤§å¯«ç¸®å¯«
      /\b[A-Z]{2,}\b/g,
      // camelCase
      /\b[a-z]+[A-Z][a-zA-Z]*\b/g,
      // snake_case
      /\b[a-z]+_[a-z_]+\b/g,
      // ç‰ˆæœ¬è™Ÿ
      /\bv?\d+\.\d+\.\d+(-[a-zA-Z0-9]+)?\b/g,
      // æ–‡ä»¶è·¯å¾‘
      /\b(?:\/|[a-zA-Z]:\\\\)[^\s<>:"|?*]*\b/g,
      // URL
      /https?:\/\/[^\s<>"]+/g,
      // æŠ€è¡“é—œéµè©
      /\b(async|await|function|class|const|let|var|import|export|return|throw|try|catch|finally|interface|type|enum|Promise|Array|Object|String|Number|Boolean)\b/g
    ];

    for (const pattern of patterns) {
      const matches = text.match(pattern);
      if (matches) {
        technicalTerms.push(...matches);
      }
    }

    // å»é‡
    return [...new Set(technicalTerms)];
  }

  /**
   * è¨ˆç®—åˆ†æ•¸
   */
  private calculateScores(tokens: DirectStarCoder2Token[], content: string): DirectStarCoder2Token[] {
    const scoredTokens: DirectStarCoder2Token[] = [];

    for (const token of tokens) {
      let score = token.score;

      // 1. æŠ€è¡“è¡“èªåŠ æ¬Š
      if (this.isTechnicalTerm(token.text)) score += 0.2;

      // 2. é•·åº¦åŠ æ¬Š
      if (token.text.length > 6) score += 0.1;
      if (token.text.length > 10) score += 0.1;

      // 3. é »ç‡åŠ æ¬Š
      const frequency = this.calculateTokenFrequency(token.text, content);
      if (frequency === 1) score += 0.2; // ç¨ç‰¹è©æ›´é‡è¦
      else if (frequency >= 5) score -= 0.1; // å¸¸è¦‹è©é™ä½é‡è¦æ€§

      // 4. çµæ§‹é‡è¦æ€§åŠ æ¬Š
      if (this.isStructuralToken(token.text)) score += 0.15;

      scoredTokens.push({
        ...token,
        score: Math.min(score, 1.0),
        relevance: this.determineRelevance(score)
      });
    }

    // æŒ‰åˆ†æ•¸æ’åº
    return scoredTokens.sort((a, b) => b.score - a.score);
  }

  /**
   * åˆ¤æ–·æ˜¯å¦ç‚ºæŠ€è¡“è¡“èª
   */
  private isTechnicalTerm(token: string): boolean {
    return /^[A-Z]{2,}$/.test(token) || // å…¨å¤§å¯«ç¸®å¯«
           /^[a-z]+[A-Z]/.test(token) || // camelCase
           /^[a-z]+_[a-z_]+$/.test(token) || // snake_case
           /^v?\d+\.\d+\.\d+/.test(token) || // ç‰ˆæœ¬è™Ÿ
           /^(async|await|function|class|const|let|var|import|export|return|throw|try|catch|finally|interface|type|enum)$/.test(token); // é—œéµå­—
  }

  /**
   * åˆ¤æ–·æ˜¯å¦ç‚ºçµæ§‹ token
   */
  private isStructuralToken(token: string): boolean {
    return /^[\{\}\(\)\[\];,<\.>]$/.test(token);
  }

  /**
   * è¨ˆç®— token é »ç‡
   */
  private calculateTokenFrequency(token: string, content: string): number {
    const regex = new RegExp(this.escapeRegExp(token), 'g');
    const matches = content.match(regex);
    return matches ? matches.length : 0;
  }

  /**
   * è½‰ç¾©æ­£å‰‡è¡¨é”å¼ç‰¹æ®Šå­—ç¬¦
   */
  private escapeRegExp(string: string): string {
    return string.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
  }

  /**
   * ç¢ºå®šç›¸é—œæ€§ç´šåˆ¥
   */
  private determineRelevance(score: number): 'high' | 'medium' | 'low' {
    if (score >= 0.8) return 'high';
    if (score >= 0.5) return 'medium';
    return 'low';
  }

  /**
   * è¨ˆç®—å¹³å‡ç½®ä¿¡åº¦
   */
  private calculateAverageConfidence(tokens: DirectStarCoder2Token[]): number {
    if (tokens.length === 0) return 0;
    const totalConfidence = tokens.reduce((sum, token) => sum + token.confidence, 0);
    return totalConfidence / tokens.length;
  }

  /**
   * ä¾¿åˆ©æ–¹æ³•ï¼šåªè¿”å›é«˜åˆ†æ•¸ tokens
   */
  async getTopTokens(content: string, limit: number = 20, minScore: number = 0.3): Promise<DirectStarCoder2Token[]> {
    const result = await this.tokenize(content);
    return result.tokens.filter(token => token.score >= minScore).slice(0, limit);
  }

  /**
   * ä¾¿åˆ©æ–¹æ³•ï¼šåªè¿”å›æŠ€è¡“ç›¸é—œ tokens
   */
  async getTechnicalTokens(content: string): Promise<DirectStarCoder2Token[]> {
    const result = await this.tokenize(content);
    return result.tokens.filter(token =>
      this.isTechnicalTerm(token.text) ||
      token.relevance === 'high'
    );
  }

  /**
   * è§£ç¢¼ token IDs å›æ–‡æœ¬
   */
  async decode(tokenIds: number[]): Promise<string> {
    if (!this.initialized) {
      throw new Error('Tokenizer not initialized. Call initialize() first.');
    }
    return await this.tokenizer.decode(tokenIds);
  }

  /**
   * ç·¨ç¢¼æ–‡æœ¬ç‚º token IDs
   */
  async encode(text: string): Promise<number[]> {
    if (!this.initialized) {
      throw new Error('Tokenizer not initialized. Call initialize() first.');
    }
    const result = await this.tokenizer(text);
    return result.input_ids.tolist()[0];
  }
}